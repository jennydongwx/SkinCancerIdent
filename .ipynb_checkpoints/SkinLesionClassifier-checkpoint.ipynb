{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision import models,transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "skin_df = pd.read_csv('data/HAM10000_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>dx</th>\n",
       "      <th>dx_type</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>localization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HAM_0000118</td>\n",
       "      <td>ISIC_0027419</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HAM_0000118</td>\n",
       "      <td>ISIC_0025030</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HAM_0002730</td>\n",
       "      <td>ISIC_0026769</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HAM_0002730</td>\n",
       "      <td>ISIC_0025661</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HAM_0001466</td>\n",
       "      <td>ISIC_0031633</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>75.0</td>\n",
       "      <td>male</td>\n",
       "      <td>ear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lesion_id      image_id   dx dx_type   age   sex localization\n",
       "0  HAM_0000118  ISIC_0027419  bkl   histo  80.0  male        scalp\n",
       "1  HAM_0000118  ISIC_0025030  bkl   histo  80.0  male        scalp\n",
       "2  HAM_0002730  ISIC_0026769  bkl   histo  80.0  male        scalp\n",
       "3  HAM_0002730  ISIC_0025661  bkl   histo  80.0  male        scalp\n",
       "4  HAM_0001466  ISIC_0031633  bkl   histo  75.0  male          ear"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skin_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the 7 types of skin lesions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nv       6705\n",
       "mel      1113\n",
       "bkl      1099\n",
       "bcc       514\n",
       "akiec     327\n",
       "vasc      142\n",
       "df        115\n",
       "Name: dx, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skin_df['dx'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For future processing, we need to convert our label to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "skin_df['cell_type_idx'] = pd.Categorical(skin_df['dx']).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    6705\n",
       "4    1113\n",
       "2    1099\n",
       "1     514\n",
       "0     327\n",
       "6     142\n",
       "3     115\n",
       "Name: cell_type_idx, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skin_df['cell_type_idx'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the data in the form of images, and we want to convert them into matrix format that works well with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to match up the image filenames with their corresponding image id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../SkinLesionClassifier_data'\n",
    "all_image_path = glob(os.path.join(data_dir, '*', '*.jpg'))\n",
    "imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x for x in all_image_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "skin_df['path'] = skin_df['image_id'].map(imageid_path_dict.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>dx</th>\n",
       "      <th>dx_type</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>localization</th>\n",
       "      <th>cell_type_idx</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HAM_0000118</td>\n",
       "      <td>ISIC_0027419</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HAM_0000118</td>\n",
       "      <td>ISIC_0025030</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HAM_0002730</td>\n",
       "      <td>ISIC_0026769</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HAM_0002730</td>\n",
       "      <td>ISIC_0025661</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HAM_0001466</td>\n",
       "      <td>ISIC_0031633</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>75.0</td>\n",
       "      <td>male</td>\n",
       "      <td>ear</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lesion_id      image_id   dx dx_type   age   sex localization  \\\n",
       "0  HAM_0000118  ISIC_0027419  bkl   histo  80.0  male        scalp   \n",
       "1  HAM_0000118  ISIC_0025030  bkl   histo  80.0  male        scalp   \n",
       "2  HAM_0002730  ISIC_0026769  bkl   histo  80.0  male        scalp   \n",
       "3  HAM_0002730  ISIC_0025661  bkl   histo  80.0  male        scalp   \n",
       "4  HAM_0001466  ISIC_0031633  bkl   histo  75.0  male          ear   \n",
       "\n",
       "   cell_type_idx  path  \n",
       "0              2  None  \n",
       "1              2  None  \n",
       "2              2  None  \n",
       "3              2  None  \n",
       "4              2  None  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skin_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fliter for unique lesion_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to see if `lesion_id` is unique and keep only unduplicated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5514, 9)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this will tell us how many images are associated with each lesion_id\n",
    "df_undup = skin_df.groupby('lesion_id').count()\n",
    "# now we filter out lesion_id's that have only one image associated with it\n",
    "df_undup = df_undup[df_undup['image_id'] == 1]\n",
    "df_undup.reset_index(inplace=True)\n",
    "df_undup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5514, 9)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_id = pd.Series(df_undup['lesion_id'].unique())\n",
    "skin_df_uniq = skin_df[skin_df['lesion_id'].isin(unique_id)]\n",
    "skin_df_uniq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in image and resize it to 100 x 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#skin_df_uniq['image'] = skin_df_uniq['path'].map(lambda x: np.asarray(Image.open(x).resize((100,100))))\n",
    "skin_df_uniq['image'] = skin_df_uniq['path'].map(lambda x: np.asarray(Image.open(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>dx</th>\n",
       "      <th>dx_type</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>localization</th>\n",
       "      <th>cell_type_idx</th>\n",
       "      <th>path</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HAM_0001396</td>\n",
       "      <td>ISIC_0025276</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>55.0</td>\n",
       "      <td>female</td>\n",
       "      <td>trunk</td>\n",
       "      <td>2</td>\n",
       "      <td>../SkinLesionClassifier_data/HAM10000_images_p...</td>\n",
       "      <td>[[[22, 13, 16], [22, 11, 15], [22, 11, 15], [2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>HAM_0007207</td>\n",
       "      <td>ISIC_0031326</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>65.0</td>\n",
       "      <td>male</td>\n",
       "      <td>back</td>\n",
       "      <td>2</td>\n",
       "      <td>../SkinLesionClassifier_data/HAM10000_images_p...</td>\n",
       "      <td>[[[202, 160, 180], [198, 159, 178], [197, 160,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>HAM_0006071</td>\n",
       "      <td>ISIC_0032343</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>70.0</td>\n",
       "      <td>female</td>\n",
       "      <td>face</td>\n",
       "      <td>2</td>\n",
       "      <td>../SkinLesionClassifier_data/HAM10000_images_p...</td>\n",
       "      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>HAM_0005612</td>\n",
       "      <td>ISIC_0024981</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>2</td>\n",
       "      <td>../SkinLesionClassifier_data/HAM10000_images_p...</td>\n",
       "      <td>[[[204, 209, 213], [205, 204, 209], [205, 203,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>HAM_0005388</td>\n",
       "      <td>ISIC_0027815</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>chest</td>\n",
       "      <td>2</td>\n",
       "      <td>../SkinLesionClassifier_data/HAM10000_images_p...</td>\n",
       "      <td>[[[0, 0, 0], [0, 0, 0], [1, 1, 1], [6, 4, 7], ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lesion_id      image_id   dx dx_type   age     sex localization  \\\n",
       "10  HAM_0001396  ISIC_0025276  bkl   histo  55.0  female        trunk   \n",
       "15  HAM_0007207  ISIC_0031326  bkl   histo  65.0    male         back   \n",
       "20  HAM_0006071  ISIC_0032343  bkl   histo  70.0  female         face   \n",
       "33  HAM_0005612  ISIC_0024981  bkl   histo  80.0    male        scalp   \n",
       "34  HAM_0005388  ISIC_0027815  bkl   histo  80.0    male        chest   \n",
       "\n",
       "    cell_type_idx                                               path  \\\n",
       "10              2  ../SkinLesionClassifier_data/HAM10000_images_p...   \n",
       "15              2  ../SkinLesionClassifier_data/HAM10000_images_p...   \n",
       "20              2  ../SkinLesionClassifier_data/HAM10000_images_p...   \n",
       "33              2  ../SkinLesionClassifier_data/HAM10000_images_p...   \n",
       "34              2  ../SkinLesionClassifier_data/HAM10000_images_p...   \n",
       "\n",
       "                                                image  \n",
       "10  [[[22, 13, 16], [22, 11, 15], [22, 11, 15], [2...  \n",
       "15  [[[202, 160, 180], [198, 159, 178], [197, 160,...  \n",
       "20  [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...  \n",
       "33  [[[204, 209, 213], [205, 204, 209], [205, 203,...  \n",
       "34  [[[0, 0, 0], [0, 0, 0], [1, 1, 1], [6, 4, 7], ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skin_df_uniq.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = skin_df_uniq.drop('cell_type_idx', axis = 1)\n",
    "target = skin_df_uniq['cell_type_idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_o, x_test_o, Y_train, Y_test = train_test_split(features, target, test_size=0.20,random_state=118)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_o = x_train_o.reset_index(drop = True)\n",
    "x_test_o = x_test_o.reset_index(drop = True)\n",
    "Y_train = Y_train.reset_index(drop = True)\n",
    "Y_test = Y_test.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalance class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed from earlier that class 'nv' has dominated significantly more than the other classes. To ease future trouble cased by imbalanced classes, we will manually create more training samples for the other classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.026525\n",
       "4    0.042847\n",
       "1    0.030605\n",
       "5    0.800045\n",
       "2    0.081387\n",
       "6    0.011789\n",
       "3    0.006801\n",
       "Name: cell_type_idx, dtype: float64"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x_train_o['cell_type_idx'].\n",
    "Y_train['cell_type_idx'].value_counts(normalize = True)\n",
    "#.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nv       3529\n",
       "akiec    3510\n",
       "bcc      3375\n",
       "df       3000\n",
       "bkl      2872\n",
       "mel      2835\n",
       "vasc     2600\n",
       "Name: dx, dtype: int64"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy fewer class to balance the number of 7 classes \n",
    "data_aug_rate = [30,25,8,100,15,0,50]\n",
    "df_train = x_train_o.copy()\n",
    "df_train['cell_type_idx'] = Y_train\n",
    "#cell_type_dict = dict(zip(x_train_o['dx'], Y_train))\n",
    "#for celltype in list(x_train_o['dx'].unique()):\n",
    "    #i = cell_type_dict[celltype]\n",
    "for i in range(7):\n",
    "    if data_aug_rate[i]:\n",
    "        df_train=df_train.append([df_train.loc[df_train['cell_type_idx'] == i,:]]*(data_aug_rate[i]-1), ignore_index=True)\n",
    "\n",
    "df_train['dx'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop('cell_type_idx', axis = 1).reset_index(drop=True)\n",
    "Y_train = df_train['cell_type_idx'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = pd.Series([int(x) for x in Y_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce bias among the pictures, we standardlize the images base on their theoretical mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_img_mean_std(df):\n",
    "    img_RGB = df['image'].values\n",
    "    RGB_vals = []\n",
    "    # for each image, get the mean for each RGB layer\n",
    "    for i in range(len(img_RGB)):\n",
    "        R = img_RGB[i][:,:,0]\n",
    "        G = img_RGB[i][:,:,1]\n",
    "        B = img_RGB[i][:,:,2]\n",
    "        RGB_vals.append([np.mean(R)/255, np.mean(G)/255, np.mean(B)/255])\n",
    "    # compute mean and std\n",
    "    RGB_vals = np.array(RGB_vals)\n",
    "    img_mean = np.mean(RGB_vals, axis = 0)\n",
    "    img_std = np.std(RGB_vals, axis = 0)\n",
    "    return img_mean, img_std\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean, train_std = compute_img_mean_std(X_train)\n",
    "test_mean, test_std = compute_img_mean_std(x_test_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77658539 0.55929676 0.58420874] [0.0890289  0.08319234 0.08973768]\n"
     ]
    }
   ],
   "source": [
    "print(train_mean, train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.79933838 0.52175165 0.53925165] [0.08701616 0.08624265 0.09363755]\n"
     ]
    }
   ],
   "source": [
    "print(test_mean, test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an image loader\n",
    "# this part is inspired \n",
    "# https://www.kaggle.com/xinruizhuang/skin-lesion-classification-acc-90-pytorch#Step-1.-Data-analysis-and-preprocessing\n",
    "class HAM10000(Dataset):\n",
    "    def __init__(self, X, Y, transform=None):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load data and get label\n",
    "        X = Image.open(self.X['path'][index])\n",
    "        #X = self.X\n",
    "        #y = torch.tensor(int(self.df['cell_type_idx'][index]))\n",
    "        y = torch.tensor(Y_train.values[index])\n",
    "\n",
    "        if self.transform:\n",
    "            X = self.transform(X)\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 100\n",
    "train_transform = transforms.Compose([transforms.Resize((input_size,input_size)),transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomVerticalFlip(),transforms.RandomRotation(20),\n",
    "                                      transforms.ColorJitter(brightness=0.1, contrast=0.1, hue=0.1),\n",
    "                                        transforms.ToTensor(),\n",
    "                                     transforms.Normalize(train_mean, train_std)])\n",
    "# define the transformation of the val images.\n",
    "test_transform = transforms.Compose([transforms.Resize((input_size,input_size)), transforms.ToTensor(),\n",
    "                                    transforms.Normalize(test_mean, test_std)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = HAM10000(X_train,Y_train, transform=train_transform)\n",
    "trainloader = DataLoader(training_set, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "testing_set = HAM10000(x_test_o, Y_test,transform=test_transform)\n",
    "testloader = DataLoader(testing_set, batch_size=32, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 100, 100])\n",
      "tensor([6, 3, 6, 3, 0, 0, 2, 4, 5, 6, 5, 5, 4, 0, 0, 5, 2, 1, 2, 0, 6, 2, 2, 4,\n",
      "        2, 0, 1, 2, 6, 0, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "## try if trainloader is working\n",
    "for i, data in enumerate(trainloader, 0):\n",
    "    # Get the inputs.\n",
    "    inputs, labels = data\n",
    "    if i == 1:\n",
    "        print(inputs.shape)\n",
    "        print(labels)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# If there are GPUs, choose the first one for computing. Otherwise use CPU.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)  \n",
    "# If 'cuda:0' is printed, it means GPU is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's try a two-layer CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc2): Linear(in_features=20000, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        ###### Fill the blank here ######\n",
    "        # conv layer\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, stride = 1, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, stride = 1, padding = 1)\n",
    "        \n",
    "        # linear layer\n",
    "        #self.fc1 = nn.Linear(20 * 8 * 8, 1280) \n",
    "        self.fc2 = nn.Linear(20000,32)\n",
    "        self.fc3 = nn.Linear(32, 7)\n",
    "       \n",
    "        \n",
    "    def forward(self, x):\n",
    "        ###### Fill the blank here ######\n",
    "        # two rounds of avg pooling\n",
    "        # avg pooling over a (2, 2) window\n",
    "        x = F.avg_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.avg_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        \n",
    "        \n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        #x = self.fc1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    " \n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x): # flatten matrix\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "net = Net()     # Create the network instance.\n",
    "net.to(device)  # Move the network parameters to the specified device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use cross-entropy as loss function.\n",
    "loss_func = nn.CrossEntropyLoss()  \n",
    "# We use stochastic gradient descent (SGD) as optimizer.\n",
    "opt = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 0, i:    99] avg mini-batch loss: 1.724\n",
      "[epoch: 0, i:   199] avg mini-batch loss: 1.531\n",
      "[epoch: 0, i:   299] avg mini-batch loss: 1.465\n",
      "[epoch: 0, i:   399] avg mini-batch loss: 1.373\n",
      "[epoch: 0, i:   499] avg mini-batch loss: 1.330\n",
      "[epoch: 0, i:   599] avg mini-batch loss: 1.306\n",
      "[epoch: 1, i:    99] avg mini-batch loss: 1.252\n",
      "[epoch: 1, i:   199] avg mini-batch loss: 1.231\n",
      "[epoch: 1, i:   299] avg mini-batch loss: 1.156\n",
      "[epoch: 1, i:   399] avg mini-batch loss: 1.162\n",
      "[epoch: 1, i:   499] avg mini-batch loss: 1.115\n",
      "[epoch: 1, i:   599] avg mini-batch loss: 1.123\n",
      "[epoch: 2, i:    99] avg mini-batch loss: 1.070\n",
      "[epoch: 2, i:   199] avg mini-batch loss: 1.040\n",
      "[epoch: 2, i:   299] avg mini-batch loss: 1.040\n",
      "[epoch: 2, i:   399] avg mini-batch loss: 1.028\n",
      "[epoch: 2, i:   499] avg mini-batch loss: 1.018\n",
      "[epoch: 2, i:   599] avg mini-batch loss: 0.972\n",
      "[epoch: 3, i:    99] avg mini-batch loss: 0.934\n",
      "[epoch: 3, i:   199] avg mini-batch loss: 0.944\n",
      "[epoch: 3, i:   299] avg mini-batch loss: 0.940\n",
      "[epoch: 3, i:   399] avg mini-batch loss: 0.907\n",
      "[epoch: 3, i:   499] avg mini-batch loss: 0.913\n",
      "[epoch: 3, i:   599] avg mini-batch loss: 0.880\n",
      "[epoch: 4, i:    99] avg mini-batch loss: 0.813\n",
      "[epoch: 4, i:   199] avg mini-batch loss: 0.850\n",
      "[epoch: 4, i:   299] avg mini-batch loss: 0.813\n",
      "[epoch: 4, i:   399] avg mini-batch loss: 0.812\n",
      "[epoch: 4, i:   499] avg mini-batch loss: 0.771\n",
      "[epoch: 4, i:   599] avg mini-batch loss: 0.786\n",
      "[epoch: 5, i:    99] avg mini-batch loss: 0.748\n",
      "[epoch: 5, i:   199] avg mini-batch loss: 0.758\n",
      "[epoch: 5, i:   299] avg mini-batch loss: 0.711\n",
      "[epoch: 5, i:   399] avg mini-batch loss: 0.773\n",
      "[epoch: 5, i:   499] avg mini-batch loss: 0.748\n",
      "[epoch: 5, i:   599] avg mini-batch loss: 0.688\n",
      "[epoch: 6, i:    99] avg mini-batch loss: 0.671\n",
      "[epoch: 6, i:   199] avg mini-batch loss: 0.699\n",
      "[epoch: 6, i:   299] avg mini-batch loss: 0.666\n",
      "[epoch: 6, i:   399] avg mini-batch loss: 0.679\n",
      "[epoch: 6, i:   499] avg mini-batch loss: 0.695\n",
      "[epoch: 6, i:   599] avg mini-batch loss: 0.667\n",
      "[epoch: 7, i:    99] avg mini-batch loss: 0.663\n",
      "[epoch: 7, i:   199] avg mini-batch loss: 0.673\n",
      "[epoch: 7, i:   299] avg mini-batch loss: 0.631\n",
      "[epoch: 7, i:   399] avg mini-batch loss: 0.616\n",
      "[epoch: 7, i:   499] avg mini-batch loss: 0.636\n",
      "[epoch: 7, i:   599] avg mini-batch loss: 0.621\n",
      "[epoch: 8, i:    99] avg mini-batch loss: 0.591\n",
      "[epoch: 8, i:   199] avg mini-batch loss: 0.625\n",
      "[epoch: 8, i:   299] avg mini-batch loss: 0.570\n",
      "[epoch: 8, i:   399] avg mini-batch loss: 0.575\n",
      "[epoch: 8, i:   499] avg mini-batch loss: 0.618\n",
      "[epoch: 8, i:   599] avg mini-batch loss: 0.574\n",
      "[epoch: 9, i:    99] avg mini-batch loss: 0.558\n",
      "[epoch: 9, i:   199] avg mini-batch loss: 0.585\n",
      "[epoch: 9, i:   299] avg mini-batch loss: 0.542\n",
      "[epoch: 9, i:   399] avg mini-batch loss: 0.552\n",
      "[epoch: 9, i:   499] avg mini-batch loss: 0.555\n",
      "[epoch: 9, i:   599] avg mini-batch loss: 0.554\n",
      "Finished Training.\n"
     ]
    }
   ],
   "source": [
    "avg_losses = []   # Avg. losses.\n",
    "epochs = 10       # Total epochs.\n",
    "print_freq = 100  # Print frequency.\n",
    "\n",
    "for epoch in range(epochs):  # Loop over the dataset multiple times.\n",
    "    running_loss = 0.0       # Initialize running loss.\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get the inputs.\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # Move the inputs to the specified device.\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients.\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Forward step.\n",
    "        outputs = net(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        \n",
    "        # Backward step.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimization step (update the parameters).\n",
    "        opt.step()\n",
    "\n",
    "        # Print statistics.\n",
    "        running_loss += loss.item()\n",
    "        if i % print_freq == print_freq - 1: # Print every several mini-batches.\n",
    "            avg_loss = running_loss / print_freq\n",
    "            print('[epoch: {}, i: {:5d}] avg mini-batch loss: {:.3f}'.format(\n",
    "                epoch, i, avg_loss))\n",
    "            avg_losses.append(avg_loss)\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(avg_losses)\n",
    "plt.xlabel('mini-batch index / {}'.format(print_freq))\n",
    "plt.ylabel('avg. mini-batch loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imshow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-8419119fc964>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdataiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GroundTruth: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%5s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'imshow' is not defined"
     ]
    }
   ],
   "source": [
    "# Check several images.\n",
    "## (SKIPPED for now)\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
    "outputs = net(images.to(device))\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 49 %\n"
     ]
    }
   ],
   "source": [
    "# Get test accuracy.\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of   bkl :  8 %\n",
      "Accuracy of    nv :  2 %\n",
      "Accuracy of    df : 18 %\n",
      "Accuracy of   mel :  0 %\n",
      "Accuracy of  vasc : 12 %\n",
      "Accuracy of   bcc : 58 %\n",
      "Accuracy of akiec :  0 %\n"
     ]
    }
   ],
   "source": [
    "# Get test accuracy for each class.\n",
    "classes = list(skin_df_uniq['dx'].unique())\n",
    "class_correct = list(0. for i in range(7))\n",
    "class_total = list(0. for i in range(7))\n",
    "\n",
    "y_label = []\n",
    "y_predict = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        \n",
    "        y_label.extend(labels.cpu().numpy())\n",
    "        y_predict.extend(np.squeeze(predicted.cpu().numpy().T))\n",
    "\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "for i in range(7):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    626\n",
       "2    189\n",
       "1    103\n",
       "4     73\n",
       "6     50\n",
       "0     36\n",
       "3     26\n",
       "dtype: int64"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_predict).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bkl       0.06      0.08      0.07        24\n",
      "          nv       0.01      0.03      0.01        36\n",
      "          df       0.10      0.19      0.13        95\n",
      "         mel       0.00      0.00      0.00         4\n",
      "        vasc       0.07      0.12      0.09        41\n",
      "         bcc       0.83      0.58      0.68       893\n",
      "       akiec       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.49      1103\n",
      "   macro avg       0.15      0.14      0.14      1103\n",
      "weighted avg       0.68      0.49      0.57      1103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_label, y_predict, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
